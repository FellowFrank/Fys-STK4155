{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc2501d",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html exercisesweek47.do.txt  -->\n",
    "<!-- dom:TITLE: Exercise week 47-48 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae5111",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Exercise week 47-48\n",
    "**November 17-28, 2025**\n",
    "\n",
    "Date: **Deadline is Friday November 28 at midnight**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef837a4",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Overarching aims of the exercises this week\n",
    "\n",
    "The exercise set this week is meant as a summary of many of the\n",
    "central elements in various machine learning algorithms we have discussed throught the semester. You don't need to answer all questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ef66b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Linear and logistic regression methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c9231",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 1:\n",
    "\n",
    "Which of the following is not an assumption of ordinary least squares linear regression?\n",
    "\n",
    "NOT an assumption -> There is a linearity between predictors/features and target/outout\n",
    "\n",
    "IS an assumption -> The inputs/features distributed according to a normal/gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acef906",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 2:\n",
    "\n",
    "The mean squared error cost function for linear regression is convex in the parameters, guaranteeing a unique global minimum. True or False? Motivate your answer.\n",
    "\n",
    "\n",
    "True, as we have an analytical solution for the minimum of the OLS method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bf02e",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 3:\n",
    "\n",
    "Which statement about logistic regression is false?\n",
    "\n",
    " Logistic regression is used for binary classification.\n",
    "\n",
    " It uses the sigmoid function to map linear scores to probabilities.\n",
    "\n",
    " False -> It has an analytical closed-form solution.\n",
    "\n",
    " Its log-loss (cross-entropy) is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab306a",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 4:\n",
    "\n",
    "Logistic regression produces a linear decision boundary in the input space. True or False? Explain.\n",
    "\n",
    "True, as it separates based on a probability if it is true ot not. i.e. we have a linear output which we then create a decision threshold on which is a line.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d695e6bb",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 5:\n",
    "\n",
    "Give two reasons why logistic regression is preferred over linear regression for binary classification.\n",
    "\n",
    "It is preferred over linear regression as it has a steep curve near the boundary and gives two classes 0 or 1 based on the boundary and the squish of everything within these bounds.\n",
    "In a more condensed form:\n",
    "- Robustness to outliers\n",
    "- Clear output and interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c398642",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fac35",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 6:\n",
    "\n",
    "Which statement is not true for fully-connected neural networks?\n",
    "\n",
    "* Without nonlinear activation functions they reduce to a single linear model.\n",
    "\n",
    "* Training relies on backpropagation using the chain rule.\n",
    "\n",
    "* A single hidden layer can approximate any continuous function on a compact set.\n",
    "\n",
    "* False -> The loss surface of a deep neural network is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed2727",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 7:\n",
    "\n",
    "Using sigmoid activations in many layers of a deep neural network can cause vanishing gradients. True or False? Explain.\n",
    "\n",
    "True, as with numbers not close to the middle of the sigmoid function has a gradient that is close to 0 creating a sequence of small numbers multiplied together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1865d",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 8:\n",
    "\n",
    "Describe the vanishing gradient problem: Why does it occur? Mention one technique to mitigate it and explain briefly.\n",
    "\n",
    "It occurs because of the multiple multiplication of small numbers because of the chain rule.\n",
    "One can mitigate it by having an optimization rule that increases the movement downwards for each update to the parameters. Such as ADAM or RMSPROP.\n",
    "\n",
    "Or one could use a different activation function such as LeakyReLU, which has a constant gradient at some size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ad1a8",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 9:\n",
    "\n",
    "Consider a fully-connected network with layer sizes $n_0$ (the input\n",
    "layer) ,$n_1$ (first hidden layer), $\\dots, n_L$, where $n_L$ is the\n",
    "output layer. Derive a general formula for the total number of\n",
    "trainable parameters (weights + biases).\n",
    "\n",
    "\n",
    "$$\n",
    "(n_0 * n_1 + n_1)+(n_1 * n_2 + n_2) + \\dots = (\\Sigma n_i n_{i+1} + \\Sigma_{i=1} n_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2ed47",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d54a83",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 10:\n",
    "\n",
    "Which of the following is not a typical property or advantage of CNNs?\n",
    "\n",
    "* Local receptive fields\n",
    "\n",
    " * Weight sharing\n",
    "\n",
    " * This is not a property -> More parameters than fully-connected layers\n",
    "\n",
    " * Pooling layers offering some translation invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aefcc46",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 11:\n",
    "\n",
    "Using zero-padding in convolutional layers can preserve the input\n",
    "spatial dimensions when using a $3 \\times 3$ kernel/filter, stride 1,\n",
    "and padding $P = 1$. True or False?\n",
    "\n",
    "True, as we we will have the same output as input. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b6806",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 12:\n",
    "\n",
    "Given input width $W$, kernel size $K$, stride S, and padding P,\n",
    "derive the formula for the output width $W_{\\text{out}} = \\frac{W - K+ 2P}{S} + 1$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a629397f",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 13:\n",
    "\n",
    "A convolutional layer has: $C_{\\text{in}}$ input channels,\n",
    "$C_{\\text{out}}$ output channels (filters) and kernel size $K_h \\times\n",
    "K_w$. Compute the number of trainable parameters including biases.\n",
    "\n",
    "\n",
    "One filter is equal to $K_h * K_w * C_{in}$,\n",
    "so the amount for all filters will be $K_h * K_w * C_{in} * C_{out}$\n",
    "\n",
    "Then we have the bias which is equal to $C_{out}$\n",
    "\n",
    "Total: $(K_h * K_w * C_{in} +1)* C_{out}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087780b2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd5f95",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 14:\n",
    "\n",
    "Which statement about simple  RNNs is false?\n",
    "\n",
    "* They maintain a hidden state updated each time step.\n",
    "\n",
    " * They use the same weight matrices at every time step.\n",
    "\n",
    " * They handle sequences of arbitrary length.\n",
    "\n",
    " * False -> They eliminate the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd70bb6d",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 15:\n",
    "\n",
    "LSTMs mitigate the vanishing gradient problem by using gating mechanisms (input, forget, output gates). True or False? Explain.\n",
    "\n",
    "True, as it fails via the chainrule multiplication it is allevated or fixed via having and additive memory of the cost function, with a forgetting parameter and input paramter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ec77a",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 16:\n",
    "\n",
    "What is Backpropagation Through Time (BPTT) and why is it required for training RNNs?\n",
    "\n",
    "As an RNN loops compared to the standard FFNN, one needs to make some changes for the backpropogation to work. \n",
    "BPTT fixes this by seeing the NN as a long chain of copies of the cell for each time, here the weights are identical. \n",
    "Calculate the Gradient for each step, sum them up and the change the Weights for this cell using this gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e01d4",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 17:\n",
    "\n",
    "What does a sliding window do? And why would we use it?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
