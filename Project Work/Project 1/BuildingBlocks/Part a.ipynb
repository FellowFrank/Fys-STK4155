{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f3c27f7",
   "metadata": {},
   "source": [
    "## Generating the dataset\n",
    "Generating the dataset for runge function from -1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aeb84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 100\n",
    "\n",
    "x = np.linspace(-1,1,n)\n",
    "y = 1/(1+25*x**2) \n",
    "y = y.reshape(n,1) \n",
    "y_noise = y + np.random.normal(0,0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4191d",
   "metadata": {},
   "source": [
    "## Part a : OLS for Runge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6680f8a",
   "metadata": {},
   "source": [
    "Write your own code (using for example the pseudoinverse function pinv from Numpy ) and\n",
    "perform a standard ordinary least square regression analysis using polynomials in x up to\n",
    "order 15 or higher. Explore the dependence on the number of data points and the polynomial\n",
    "degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b4e9af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, p, intercept=False):\n",
    "    n = len(x)\n",
    "    k = 0\n",
    "    if intercept:\n",
    "        X = np.zeros((n, p + 1))\n",
    "        X[:, 0] = 1\n",
    "        k += 1\n",
    "    else:\n",
    "        X = np.zeros((n, p))\n",
    "\n",
    "    for i in range(1, p +1):\n",
    "        X[:, i + k-1] = x**i \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622712c3",
   "metadata": {},
   "source": [
    "#### Derivation of the OLS analytical solution\n",
    "Using this relationship \n",
    "$$\n",
    "\\frac{\\partial \\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)^T\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)}{\\partial \\boldsymbol{s}} = -2\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)^T\\boldsymbol{A},\n",
    "$$\n",
    "\n",
    "and noting that the minimum point is where this derivate is equal to 0 gives\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "0 &= -2 (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta})^T \\boldsymbol{X} \\\\\n",
    "0 &= y^T X - \\theta^T X^T X \\\\\n",
    "\\theta^T &= y^T X (X^T X) ^{-1}\\\\\n",
    "\\theta &= (X^T X)^{-1} X^T y\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba1536f",
   "metadata": {},
   "source": [
    "### Code\n",
    "OLS analytical solution using pseudo pinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c16ec0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinary Least Squares Analytical solution\n",
    "def OLS_parameters(X, y):\n",
    "    return np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "\n",
    "def MSE(y, y_pred):\n",
    "    return np.mean((y - y_pred)**2)\n",
    "\n",
    "def R2(y, y_pred):\n",
    "    return 1 - (np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674dd54",
   "metadata": {},
   "source": [
    "Analysis with polynomials in x up to 15 degrees where we take a train-test split to see how the trained model performs on unseen data. Where we do the scaling since x^1 and x^15 are quite far between, so the floating term operations on the computer becomes more ill defined or close to zero. So even though the solution is analytical we will necessarily have some approximation since we are using a computer and the numbers usually wont have few floating points. We also subtract the mean from the y values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f633d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m plot_train = {\u001b[33m\"\u001b[39m\u001b[33mmse\u001b[39m\u001b[33m\"\u001b[39m:[],\n\u001b[32m     10\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m\"\u001b[39m: []}\n\u001b[32m     11\u001b[39m beta_norms = []\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m degrees = \u001b[43mnp\u001b[49m.arange(\u001b[32m1\u001b[39m, \u001b[32m16\u001b[39m)\n\u001b[32m     14\u001b[39m y_centered = y - y.mean()\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m degree \u001b[38;5;129;01min\u001b[39;00m degrees:\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "plot_predict = {\"mse\" :[],\n",
    "                \"r2\" : []}\n",
    "plot_train = {\"mse\":[],\n",
    "              \"r2\": []}\n",
    "beta_norms = []\n",
    "degrees = np.arange(1, 16)\n",
    "\n",
    "y_centered = y - y.mean()\n",
    "\n",
    "for degree in degrees:\n",
    "    X = polynomial_features(x, degree)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_centered, test_size=0.2, random_state=57)\n",
    "    \n",
    "    # Scaling the data for numerical stability\n",
    "    #scaler = StandardScaler()\n",
    "    #X_train = scaler.fit_transform(X_train)\n",
    "    #X_test = scaler.transform(X_test)\n",
    "\n",
    "    beta = OLS_parameters(X_train, y_train)\n",
    "    beta_norms.append(beta)\n",
    "\n",
    "    y_pred = X_test @ beta\n",
    "    y_train_pred = X_train @ beta\n",
    "    mse_predict = MSE(y_test, y_pred)\n",
    "    mse_train = MSE(y_train, y_train_pred)\n",
    "    r2_predict = R2(y_test, y_pred)\n",
    "    r2_train = R2(y_train, y_train_pred)\n",
    "    \n",
    "    plot_predict[\"mse\"].append(mse_predict)\n",
    "    plot_predict[\"r2\"].append(r2_predict)\n",
    "    plot_train[\"mse\"].append(mse_train)\n",
    "    plot_train[\"r2\"].append(r2_train)\n",
    "\n",
    "fig = plt.figure(figsize=(22, 14))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "fig.suptitle('Polynomial Regression Analysis', fontsize=20)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0]) \n",
    "ax2 = fig.add_subplot(gs[0, 1]) \n",
    "ax3 = fig.add_subplot(gs[1, :]) \n",
    "\n",
    "ax1.plot(degrees, plot_predict[\"mse\"], label=\"Test Data\", marker='o', color='royalblue')\n",
    "ax1.plot(degrees, plot_train[\"mse\"], label=\"Train Data\", marker='o', linestyle='--', color='darkorange')\n",
    "ax1.set_xlabel(\"Model Complexity (Polynomial Degree)\", fontsize=12)\n",
    "ax1.set_ylabel(\"Mean Squared Error (MSE)\", fontsize=12)\n",
    "ax1.set_title(\"Bias-Variance Tradeoff: MSE\", fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "# Set a y-limit to keep the plot readable, as MSE can explode for high degrees\n",
    "\n",
    "\n",
    "# --- Plot 2: R-squared (on the second axis, ax2) ---\n",
    "ax2.plot(degrees, plot_predict[\"r2\"], label=\"Test Data\", marker='o', color='royalblue')\n",
    "ax2.plot(degrees, plot_train[\"r2\"], label=\"Train Data\", marker='o', linestyle='--', color='darkorange')\n",
    "ax2.set_xlabel(\"Model Complexity (Polynomial Degree)\", fontsize=12)\n",
    "ax2.set_ylabel(\"R-squared ($R^2$)\", fontsize=12)\n",
    "ax2.set_title(\"Bias-Variance Tradeoff: R-squared\", fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "# R2 is bounded by 1, negative values indicate a very poor fit\n",
    "\n",
    "\n",
    "ax3.scatter(x, y_centered, label='Training Data Points', color='gray', alpha=0.6, s=40)\n",
    "\n",
    "\n",
    "plotted_degrees = [1, 3, 6, 9, 12, 15]\n",
    "\n",
    "for degree in plotted_degrees:\n",
    "    X_full = polynomial_features(x, degree)\n",
    "    scaler = StandardScaler()\n",
    "    X_full_scaled = scaler.fit_transform(X_full)\n",
    "    y_pred = X_full_scaled @ beta_norms[degree - 1]\n",
    "    \n",
    "    ax3.plot(x, y_pred, label=f'Degree {degree} Fit', linewidth=2.5)\n",
    "\n",
    "ax3.set_xlabel(\"Feature (x)\", fontsize=12)\n",
    "ax3.set_ylabel(\"Target (y)\", fontsize=12)\n",
    "ax3.set_title(\"Polynomial Function Fits\", fontsize=16)\n",
    "ax3.legend(fontsize=11)\n",
    "ax3.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "# --- Save and show the final combined plot ---\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(\"Combined_Analysis_3_Plots.png\")\n",
    "print(\"Plot saved as Combined_Analysis_3_Plots.png\")\n",
    "plt.show()\n",
    "\n",
    "# Adjust layout to prevent titles/labels from overlapping\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure to a file and display it\n",
    "plt.savefig(\"OLS_BiasVariance_SideBySide.png\")\n",
    "print(\"Plot saved as OLS_BiasVariance_SideBySide.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160066d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fd2c0d4",
   "metadata": {},
   "source": [
    "### Data points and the polynomial degree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c80566a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
