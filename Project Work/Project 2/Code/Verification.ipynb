{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea64d33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparing Gradients (L2 norm of difference) ---\n",
      "Layer 0:\n",
      "  W_diff: 1.2119809160162261e-17\n",
      "  b_diff: 9.670986140218852e-17\n",
      "  Gradients match!\n",
      "Layer 1:\n",
      "  W_diff: 2.3351598139465184e-17\n",
      "  b_diff: 1.0203949608775757e-16\n",
      "  Gradients match!\n",
      "Layer 2:\n",
      "  W_diff: 7.217888169571747e-18\n",
      "  b_diff: 0.0\n",
      "  Gradients match!\n",
      "\n",
      "--- Comparing Gradients (L2 norm of difference) ---\n",
      "Layer 0:\n",
      "  W_diff: 1.3444106938820255e-17\n",
      "  b_diff: 8.708242736640235e-17\n",
      "  Gradients match!\n",
      "Layer 1:\n",
      "  W_diff: 2.7305707463421613e-17\n",
      "  b_diff: 9.464432931723923e-17\n",
      "  Gradients match!\n",
      "Layer 2:\n",
      "  W_diff: 1.033609230195259e-17\n",
      "  b_diff: 1.1102230246251565e-16\n",
      "  Gradients match!\n",
      "\n",
      "--- Comparing Gradients (L2 norm of difference) ---\n",
      "Layer 0:\n",
      "  W_diff: 3.436319130918789e-18\n",
      "  b_diff: 1.6719227153954813e-17\n",
      "  Gradients match!\n",
      "Layer 1:\n",
      "  W_diff: 2.0852790466893888e-16\n",
      "  b_diff: 1.0460253102363242e-16\n",
      "  Gradients match!\n",
      "Layer 2:\n",
      "  W_diff: 3.477763656540197e-16\n",
      "  b_diff: 0.0\n",
      "  Gradients match!\n",
      "\n",
      "--- Simple Training Example ---\n",
      "Epoch 0, Cost: 0.22636370801739145\n",
      "Epoch 20, Cost: 0.18984527241855736\n",
      "Epoch 40, Cost: 0.16302695996854466\n",
      "Epoch 60, Cost: 0.14332613193060978\n",
      "Epoch 80, Cost: 0.12885105275750808\n",
      "Epoch 100, Cost: 0.11821436888452212\n",
      "Training complete. Final Cost: 0.11821436888452212\n",
      "Cost reduced: True\n"
     ]
    }
   ],
   "source": [
    "# Verified code section\n",
    "from Functions import NeuralNetwork, mse, der_mse, ReLU, der_ReLU, linear, der_linear, dataset, LeakyReLu, der_LeakyReLu, sigmoid, der_sigmoid\n",
    "import numpy as np\n",
    "# 1. Create a dataset\n",
    "n_samples = 100\n",
    "x, y_true, y_noise = dataset(n_samples)\n",
    "X = x.reshape(-1, 1) # Feature matrix (n_samples, 1)\n",
    "\n",
    "# 2. Define Network Architecture\n",
    "n_inputs = 1\n",
    "layer_sizes = [20, 10, 1] # Two hidden layers, one output node\n",
    "activations = [ReLU, ReLU, linear]\n",
    "activation_ders = [der_ReLU, der_ReLU, der_linear]\n",
    "cost_func = mse\n",
    "cost_der_func = der_mse\n",
    "\n",
    "# 3. Initialize the Neural Network\n",
    "for activation, activation_der in [(ReLU, der_ReLU), (LeakyReLu, der_LeakyReLu), (sigmoid, der_sigmoid)]:\n",
    "    nn = NeuralNetwork(\n",
    "        network_input_size=n_inputs,\n",
    "        layer_output_sizes=layer_sizes,\n",
    "        activation_funcs=[activation]*2 + [linear],\n",
    "        activation_ders=[activation_der]*2 + [der_linear],\n",
    "        cost_fun=cost_func,\n",
    "        cost_der=cost_der_func,\n",
    "        seed=42\n",
    "    )\n",
    "    # 4. Test the manual gradient\n",
    "    manual_grads = nn.compute_gradient(X, y_noise)\n",
    "    auto_grads = nn.autograd_gradient(X, y_noise)\n",
    "    print(\"\\n--- Comparing Gradients (L2 norm of difference) ---\")\n",
    "    for i in range(len(manual_grads)):\n",
    "        manual_W, manual_b = manual_grads[i]\n",
    "        auto_W, auto_b = auto_grads[i]\n",
    "\n",
    "        diff_W = np.linalg.norm(manual_W - auto_W)\n",
    "        diff_b = np.linalg.norm(manual_b - auto_b)\n",
    "\n",
    "        print(f\"Layer {i}:\")\n",
    "        print(f\"  W_diff: {diff_W}\")\n",
    "        print(f\"  b_diff: {diff_b}\")\n",
    "\n",
    "        if diff_W > 1e-10 or diff_b > 1e-10:\n",
    "            print(f\"  WARNING: Large difference found in layer {i}!\")\n",
    "        else:\n",
    "            print(f\"  Gradients match!\")\n",
    "# 5. Testing Training loop\n",
    "print(\"\\n--- Simple Training Example ---\")\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "initial_cost = nn.cost(X, y_noise)\n",
    "print(f\"Epoch 0, Cost: {initial_cost}\")\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Use autograd gradient (or manual_grads, they should be the same)\n",
    "    grads = nn.compute_gradient(X, y_noise)\n",
    "    \n",
    "    # Update weights\n",
    "    grads = [(dW * learning_rate, db * learning_rate) for dW, db in grads]\n",
    "    nn.update_weights(grads)\n",
    "    \n",
    "    if (i+1) % 20 == 0:\n",
    "        cost_val = nn.cost(X, y_noise)\n",
    "        print(f\"Epoch {i+1}, Cost: {cost_val}\")\n",
    "        \n",
    "final_cost = nn.cost(X, y_noise)\n",
    "print(f\"Training complete. Final Cost: {final_cost}\")\n",
    "print(f\"Cost reduced: {initial_cost > final_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c6e1d0",
   "metadata": {},
   "source": [
    "### Testing Regression network against Scikit-learn MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91fc9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simple Training Comparison with scikit-learn ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anton\\Documents\\UiO Fag\\FYS-STK4155 Anvendt dataanalyse og maskinlæring\\Eget Arbeid\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1771: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your NeuralNetwork Test MSE: 0.071445\n",
      "SKLearn MLPRegressor Test MSE: 0.071389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anton\\Documents\\UiO Fag\\FYS-STK4155 Anvendt dataanalyse og maskinlæring\\Eget Arbeid\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Verified code section\n",
    "from Functions import NeuralNetwork, mse, der_mse, ReLU, der_ReLU, linear,sigmoid, der_sigmoid, der_linear, dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 1. Create a dataset\n",
    "n_samples = 100\n",
    "x, y_true, y_noise = dataset(n_samples)\n",
    "X = x.reshape(-1, 1) # Feature matrix (n_samples, 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = y_true - np.mean(y_true)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 2. Define Network Architecture\n",
    "n_inputs = 1\n",
    "layer_sizes = [20, 10, 1] # Two hidden layers, one output node\n",
    "activations = [sigmoid, sigmoid, linear]\n",
    "activation_ders = [der_sigmoid, der_sigmoid, der_linear]\n",
    "cost_func = mse\n",
    "cost_der_func = der_mse\n",
    "\n",
    "# 3. Initialize the Neural Network\n",
    "nn = NeuralNetwork(\n",
    "    network_input_size=n_inputs,\n",
    "    layer_output_sizes=layer_sizes,\n",
    "    activation_funcs=activations,\n",
    "    activation_ders=activation_ders,\n",
    "    cost_fun=cost_func,\n",
    "    cost_der=cost_der_func,\n",
    "    seed=42\n",
    ")\n",
    "# 5. Testing Training loop\n",
    "print(\"--- Simple Training Comparison with scikit-learn ---\")\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Use autograd gradient (or manual_grads, they should be the same)\n",
    "    grads = nn.compute_gradient(X_train, y_train)\n",
    "    \n",
    "    # Update weights\n",
    "    grads = [(dW * learning_rate, db * learning_rate) for dW, db in grads]\n",
    "    nn.update_weights(grads)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "final_cost = nn.cost(X_test, y_test)\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "# store models for later use\n",
    "dnn = MLPRegressor(hidden_layer_sizes=(20,10), activation='logistic', solver='sgd',\n",
    "                             learning_rate_init=learning_rate, learning_rate='constant', max_iter=epochs, random_state=42,\n",
    "                             batch_size=X_train.shape[0],\n",
    "                             alpha=0.000001,\n",
    "                             tol=1e-10000,\n",
    "                             early_stopping=False, verbose=False)\n",
    "dnn.fit(X_train, y_train)\n",
    "\n",
    "# Get Test MSE from your nn\n",
    "\n",
    "print(f\"Your NeuralNetwork Test MSE: {final_cost:.6f}\")\n",
    "\n",
    "# Get Test MSE from scikit-learn's dnn\n",
    "dnn_pred = dnn.predict(X_test)\n",
    "dnn_test_mse = mse(y_test, dnn_pred)\n",
    "print(f\"SKLearn MLPRegressor Test MSE: {dnn_test_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd989b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
