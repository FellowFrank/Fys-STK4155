\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{rep2}
\citation{project1}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\@writefile{toc}{\contentsline {title}{Implementation of a Feed Forward Neural Network, studied on Sparse Regression and MNIST Classification}{1}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Methods}{1}{section*.4}\protected@file@percent }
\newlabel{section:methods}{{II}{1}{}{section*.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Data Preparation}{1}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Regression: Runge's Function}{1}{section*.6}\protected@file@percent }
\citation{mnist}
\citation{rep2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Classification: MNIST Dataset}{2}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B}FFNN Method}{2}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Prediction}{2}{section*.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A simple Feed Forward Neural Network (FFNN) architecture with one input layer, one hidden layer, and one output layer. Each neuron in a layer is connected to every neuron in the subsequent layer through weights ($W$) and biases ($b$).}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:ffnn-arch}{{1}{2}{A simple Feed Forward Neural Network (FFNN) architecture with one input layer, one hidden layer, and one output layer. Each neuron in a layer is connected to every neuron in the subsequent layer through weights ($W$) and biases ($b$)}{figure.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Learning}{2}{section*.10}\protected@file@percent }
\citation{NeuralNetworks}
\citation{project1}
\citation{hastie}
\citation{project1}
\citation{hastie}
\citation{project1}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Hyperparameters}{3}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Different Building Blocks}{3}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Cost Functions}{3}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Activation Functions}{3}{section*.14}\protected@file@percent }
\citation{compfys}
\citation{project1}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{rep2}
\citation{NeuralNetworks}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Optimization Algorithms}{4}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Image Classification}{4}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Output layer}{4}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Cost Function}{4}{section*.18}\protected@file@percent }
\citation{mehta2023softmax}
\citation{rep2}
\citation{numpy}
\citation{sklearn_api}
\citation{matplotlib}
\citation{rep2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Learning}{5}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Implementation and Code}{5}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}FFNN Implementation}{5}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Testing and Comparison}{5}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {F}Use of AI tools}{5}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Results and Discussion}{5}{section*.24}\protected@file@percent }
\newlabel{section:results}{{III}{5}{}{section*.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Training the FFNN for Regression}{6}{section*.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Heatmap showing that the FFNN without any optimizations is struggling to learn anything useful after 1000 epochs of training.}}{6}{figure.2}\protected@file@percent }
\newlabel{fig:figure1}{{2}{6}{Heatmap showing that the FFNN without any optimizations is struggling to learn anything useful after 1000 epochs of training}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training and test MSE over 20000 epochs for a FFNN with 2 hidden layers and 50 nodes per layer using plain gradient descent with a learning rate of 0.1 and sigmoid activation function.}}{6}{figure.3}\protected@file@percent }
\newlabel{fig:long-training}{{3}{6}{Training and test MSE over 20000 epochs for a FFNN with 2 hidden layers and 50 nodes per layer using plain gradient descent with a learning rate of 0.1 and sigmoid activation function}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Heatmap showing that the choice of optimization algorithm has a significant impact on the FFNN's performance in learning.}}{6}{figure.4}\protected@file@percent }
\newlabel{fig:optimization-importance}{{4}{6}{Heatmap showing that the choice of optimization algorithm has a significant impact on the FFNN's performance in learning}{figure.4}{}}
\citation{project1}
\citation{project1}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Heatmap showing that change in one hyperparameter can imply a change in the optimal value of another hyperparameter.}}{7}{figure.5}\protected@file@percent }
\newlabel{fig:freezing-problems}{{5}{7}{Heatmap showing that change in one hyperparameter can imply a change in the optimal value of another hyperparameter}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}A deceptive search tuning and regularization on sparse data}{7}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}No Regularization}{7}{section*.27}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Best hyperparameters found for the FFNN regression task without regularization}}{7}{table.1}\protected@file@percent }
\newlabel{tab:hyperparameters}{{I}{7}{Best hyperparameters found for the FFNN regression task without regularization}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces FFNN regression results without regularization on noisy data.}}{7}{figure.6}\protected@file@percent }
\newlabel{fig:ffnn-no-regularization}{{6}{7}{FFNN regression results without regularization on noisy data}{figure.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}$L1$ Regularization}{7}{section*.28}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Hyperparameters used for the neural network with $L1$ regularization}}{8}{table.2}\protected@file@percent }
\newlabel{tab:hyperparameters}{{II}{8}{Hyperparameters used for the neural network with $L1$ regularization}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces FFNN regression results with L1 regularization on noisy data.}}{8}{figure.7}\protected@file@percent }
\newlabel{fig:ffnn-l1-regularization}{{7}{8}{FFNN regression results with L1 regularization on noisy data}{figure.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}$L2$ Regularization}{8}{section*.29}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Hyperparameters used for the neural network with $L2$ regularization}}{8}{table.3}\protected@file@percent }
\newlabel{tab:hyperparameters}{{III}{8}{Hyperparameters used for the neural network with $L2$ regularization}{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Final comparison of the three FFNN regression models on new data.}}{8}{figure.8}\protected@file@percent }
\newlabel{fig:final-comparison-regression}{{8}{8}{Final comparison of the three FFNN regression models on new data}{figure.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}Final Comparison regression}{8}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Image Classification}{9}{section*.31}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Hyperparameters used for the neural network training.}}{9}{table.4}\protected@file@percent }
\newlabel{tab:hyperparameters}{{IV}{9}{Hyperparameters used for the neural network training}{table.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The training and test accuracy and loss for our MNIST classification model}}{9}{figure.9}\protected@file@percent }
\newlabel{fig:mnist-accuracy-loss}{{9}{9}{The training and test accuracy and loss for our MNIST classification model}{figure.9}{}}
\citation{project1}
\bibdata{mainNotes,biblio}
\bibcite{rep2}{{1}{2025{}}{{Torgersen}}{{}}}
\bibcite{project1}{{2}{2025{}}{{Torgersen}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Heatmap of the confusion matrix for our MNIST classification model}}{10}{figure.10}\protected@file@percent }
\newlabel{fig:mnist-confusion-matrix}{{10}{10}{Heatmap of the confusion matrix for our MNIST classification model}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Misclassified 5 as 3 from the MNIST test set}}{10}{figure.11}\protected@file@percent }
\newlabel{fig:mnist-misclassified-examples}{{11}{10}{Misclassified 5 as 3 from the MNIST test set}{figure.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}further work}{10}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{10}{section*.33}\protected@file@percent }
\newlabel{section:conclusion}{{V}{10}{}{section*.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{10}{section*.34}\protected@file@percent }
\bibcite{mnist}{{3}{1998}{{LeCun\ \emph  {et~al.}}}{{LeCun, Cortes,\ and\ Burges}}}
\bibcite{NeuralNetworks}{{4}{2015}{{Nielsen}}{{}}}
\bibcite{hastie}{{5}{2009}{{Hastie\ \emph  {et~al.}}}{{Hastie, R.Tibshirani,\ and\ J.Friedman}}}
\bibcite{compfys}{{6}{2015}{{Hjorth-Jensen}}{{}}}
\bibcite{Goodfellow}{{7}{2016}{{Goodfellow\ \emph  {et~al.}}}{{Goodfellow, Bengio,\ and\ Courville}}}
\bibcite{mehta2023softmax}{{8}{2023}{{Mehta}}{{}}}
\bibcite{numpy}{{9}{2020}{{Harris\ \emph  {et~al.}}}{{Harris, Millman, van~der Walt, Gommers, Virtanen, Cournapeau, Wieser, Taylor, Berg, Smith, Kern, Picus, Hoyer, van Kerkwijk, Brett, Haldane, del R{\'{i}}o, Wiebe, Peterson, G{\'{e}}rard-Marchant, Sheppard, Reddy, Weckesser, Abbasi, Gohlke,\ and\ Oliphant}}}
\bibcite{sklearn_api}{{10}{2013}{{Buitinck\ \emph  {et~al.}}}{{Buitinck, Louppe, Blondel, Pedregosa, Mueller, Grisel, Niculae, Prettenhofer, Gramfort, Grobler, Layton, VanderPlas, Joly, Holt,\ and\ Varoquaux}}}
\bibcite{matplotlib}{{11}{2007}{{Hunter}}{{}}}
\bibstyle{apsrev4-1}
\citation{REVTEX41Control}
\citation{apsrev41Control}
\newlabel{LastBibItem}{{11}{11}{}{section*.34}{}}
\newlabel{LastPage}{{}{11}{}{}{}}
\gdef \@abspage@last{11}
