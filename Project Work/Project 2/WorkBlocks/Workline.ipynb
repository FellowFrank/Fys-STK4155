{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e310acf7",
   "metadata": {},
   "source": [
    "Goal of the research paper - what specific problem is being analyzed and in what way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7876a055",
   "metadata": {},
   "source": [
    "Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ef25b",
   "metadata": {},
   "source": [
    "## A\n",
    "### A.1\n",
    "To use the gradient machinery we need the cost/loss functions with respective gradients:\n",
    "- Mean Squared Error\n",
    "- Log loss, with and without $L_1$ and $L_2$\n",
    "- The multiclass cross entropy cost/loss function\n",
    "\n",
    "Needs to be explained in the methods section \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccfcb4c",
   "metadata": {},
   "source": [
    "#### Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2973e00",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c92875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Code section \n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def cross_entropy(predict, target):\n",
    "    return np.sum(-target * np.log(predict))\n",
    "# see https://medium.com/data-science/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1\n",
    "\n",
    "def log_loss(y_true, y_pred, regularization=None, weights=None, lambda_reg=0.01):\n",
    "    # Clipping it using epsilon to avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    m = len(y_true)\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    if regularization == \"L1\" and weights is not None:\n",
    "        loss += (lambda_reg / (2 * m)) * np.sum(np.abs(weights))\n",
    "    elif regularization == \"L2\" and weights is not None:\n",
    "        loss += (lambda_reg / (2 * m)) * np.sum(np.square(weights))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ae351",
   "metadata": {},
   "source": [
    "#### Results - comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f4ef18",
   "metadata": {},
   "source": [
    "### A.2 - Activation Functions\n",
    "Set up the expression and their first derivative for the following activation functions:\n",
    "- Sigmoid\n",
    "- RELU\n",
    "- Leaky RELU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b51787",
   "metadata": {},
   "source": [
    "#### Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabaf4d1",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98890695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def ReLu(z):\n",
    "    return np.where(z > 0, z, 0)\n",
    "\n",
    "def der_ReLu(X):\n",
    "    return np.where(X > 0, 1, 0)\n",
    "\n",
    "def LeakyReLu(z, alpha=0.01):\n",
    "    return np.where(z > 0, z, alpha * z)\n",
    "\n",
    "def der_LeakyReLu(X, alpha=0.01):\n",
    "    return np.where(X > 0, 1, alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859acff9",
   "metadata": {},
   "source": [
    "#### Results - comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be566c5c",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d6a7dc",
   "metadata": {},
   "source": [
    "Use only the mean-squared error as cost function (no regularization terms) and \n",
    "write an FFNN code for a regression problem with a flexible number of hidden\n",
    "layers and nodes using only the Sigmoid function as activation function for\n",
    "the hidden layers. Initialize the weights using a normal\n",
    "distribution. How would you initialize the biases? And which\n",
    "activation function would you select for the final output layer?\n",
    "And how would you set up your design/feature matrix? Hint: does it have to represent a polynomial approximation as you did in project 1? \n",
    "\n",
    "Train your network and compare the results with those from your OLS\n",
    "regression code from project 1 using the one-dimensional Runge\n",
    "function.  When comparing your neural network code with the OLS\n",
    "results from project 1, use the same data sets which gave you the best\n",
    "MSE score. Moreover, use the polynomial order from project 1 that gave you the\n",
    "best result.  Compare these results with your neural network with one\n",
    "and two hidden layers using $50$ and $100$ hidden nodes, respectively.\n",
    "\n",
    "Comment your results and give a critical discussion of the results\n",
    "obtained with the OLS code from project 1 and your own neural network\n",
    "code.  Make an analysis of the learning rates employed to find the\n",
    "optimal MSE score. Test both stochastic gradient descent\n",
    "with RMSprop and ADAM and plain gradient descent with different\n",
    "learning rates.\n",
    "\n",
    "You should, as you did in project 1, scale your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce5ecf",
   "metadata": {},
   "source": [
    "### Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043c8a1",
   "metadata": {},
   "source": [
    "### Code\n",
    "Should i use the same sparse data-set as i did in the earlier report or perhaps change it up? Maybe have an example on how it works on a large dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498a11f",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "We do not extract a polynomial structure for the input variables in the neural network as we let the neural network find out what the structure should be for best predicting the out put variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1a296c",
   "metadata": {},
   "source": [
    "## C\n",
    "Test the code in B against Scikit-learn, take training time and results, maybe also keras and pytorch.\n",
    "\n",
    "Test that the derivatives are correct using autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f04ecd",
   "metadata": {},
   "source": [
    "### Methods Code Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c926a36",
   "metadata": {},
   "source": [
    "## D\n",
    "Different activation functions for the hidden layers, Sigmoid, Relu and Leaky Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9286ec",
   "metadata": {},
   "source": [
    "### Results \n",
    "Bias variance trade-off analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c6a7d1",
   "metadata": {},
   "source": [
    "## E\n",
    "Testing different hyperparameter $\\lambda, \\eta, N(Layers), N(Height)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be74314",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f086a2a6",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6830ed",
   "metadata": {},
   "source": [
    "# Classification Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472cb9e",
   "metadata": {},
   "source": [
    "## F\n",
    "Change the cost function in b,d,e and perform classification analysis on MNIST problem. Evaluate the results using the Accuracy score, with a critical analysis on the parameters, activation functions and architecture of the network. Compare the results with similar results from Scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912541b",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00452d8",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8485829",
   "metadata": {},
   "source": [
    "## G\n",
    "Summarization of all the algorithms and the results to give a critical evaluation of their pros and cons. Which algorithm worked best for the regression case and which is best for the classification case  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b641c",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf39b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a48d911e",
   "metadata": {},
   "source": [
    "# CheckList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c443e6",
   "metadata": {},
   "source": [
    "## Summary of methods to implement and analyze\n",
    "\n",
    "**Required Implementation:**\n",
    "1. Reuse the regression code and results from project 1, these will act as a benchmark for seeing how suited a neural network is for this regression task.\n",
    "\n",
    "2. Implement a neural network with\n",
    "\n",
    "  * A flexible number of layers\n",
    "\n",
    "  * A flexible number of nodes in each layer\n",
    "\n",
    "  * A changeable activation function in each layer (Sigmoid, ReLU, LeakyReLU, as well as Linear and Softmax)\n",
    "\n",
    "  * A changeable cost function, which will be set to MSE for regression and cross-entropy for multiple-classification\n",
    "\n",
    "  * An optional L1 or L2 norm of the weights and biases in the cost function (only used for computing gradients, not interpretable metrics)\n",
    "\n",
    "3. Implement the back-propagation algorithm to compute the gradient of your neural network\n",
    "\n",
    "4. Reuse the implementation of Plain and Stochastic Gradient Descent from Project 1 (and adapt the code to work with the your neural network)\n",
    "\n",
    "  * With no optimization algorithm\n",
    "\n",
    "  * With RMS Prop\n",
    "\n",
    "  * With ADAM\n",
    "\n",
    "5. Implement scaling and train-test splitting of your data, preferably using sklearn\n",
    "\n",
    "6. Implement and compute metrics like the MSE and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8cc96",
   "metadata": {},
   "source": [
    "### Required Analysis:\n",
    "\n",
    "1. Briefly show and argue for the advantages and disadvantages of the methods from Project 1.\n",
    "\n",
    "2. Explore and show the impact of changing the number of layers, nodes per layer, choice of activation function, and inclusion of L1 and L2 norms. Present only the most interesting results from this exploration. 2D Heatmaps will be good for this: Start with finding a well performing set of hyper-parameters, then change two at a time in a range that shows good and bad performance.\n",
    "\n",
    "3. Show and argue for the advantages and disadvantages of using a neural network for regression on your data\n",
    "\n",
    "4. Show and argue for the advantages and disadvantages of using a neural network for classification on your data\n",
    "\n",
    "5. Show and argue for the advantages and disadvantages of the different gradient methods and learning rates when training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbf3039",
   "metadata": {},
   "source": [
    "### Optional (Note that you should include at least two of these in the report):\n",
    "\n",
    "1. Implement Logistic Regression as simple classification model case (equivalent to a Neural Network with just the output layer)\n",
    "\n",
    "2. Compute the gradient of the neural network with autograd, to show that it gives the same result as your hand-written backpropagation.\n",
    "\n",
    "3. Compare your results with results from using a machine-learning library like pytorch (https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\n",
    "\n",
    "4. Use a more complex classification dataset instead, like the fashion MNIST (see <https://www.kaggle.com/datasets/zalando-research/fashionmnist>)\n",
    "\n",
    "5. Use a more complex regression dataset instead, like the two-dimensional Runge function $f(x,y)=\\left[(10x - 5)^2 + (10y - 5)^2 + 1 \\right]^{-1}$, or even more complicated two-dimensional functions (see the supplementary material of <https://www.nature.com/articles/s41467-025-61362-4> for an extensive list of two-dimensional functions). \n",
    "\n",
    "6. Compute and interpret a confusion matrix of your best classification model (see <https://www.researchgate.net/figure/Confusion-matrix-of-MNIST-and-F-MNIST-embeddings_fig5_349758607>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05da7a6",
   "metadata": {},
   "source": [
    "## Background literature\n",
    "\n",
    "1. The text of Michael Nielsen is highly recommended, see Nielsen's book at <http://neuralnetworksanddeeplearning.com/>. It is an excellent read.\n",
    "\n",
    "2. Goodfellow, Bengio and Courville, Deep Learning at <https://www.deeplearningbook.org/>. Here we recommend chapters 6, 7 and 8\n",
    "\n",
    "3. Raschka et al. at <https://sebastianraschka.com/blog/2022/ml-pytorch-book.html>. Here we recommend chapters 11, 12 and 13."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ba8fd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
