{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6632a0",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html exercisesweek37.do.txt  -->\n",
    "<!-- dom:TITLE: Exercises week 37 -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82705c4f",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Exercises week 37\n",
    "\n",
    "**Implementing gradient descent for Ridge and ordinary Least Squares Regression**\n",
    "\n",
    "Date: **September 8-12, 2025**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921bf331",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Learning goals\n",
    "\n",
    "After having completed these exercises you will have:\n",
    "\n",
    "1. Your own code for the implementation of the simplest gradient descent approach applied to ordinary least squares (OLS) and Ridge regression\n",
    "\n",
    "2. Be able to compare the analytical expressions for OLS and Ridge regression with the gradient descent approach\n",
    "\n",
    "3. Explore the role of the learning rate in the gradient descent approach and the hyperparameter $\\lambda$ in Ridge regression\n",
    "\n",
    "4. Scale the data properly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adff65d5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple one-dimensional second-order polynomial\n",
    "\n",
    "We start with a very simple function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70418b3d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "f(x)= 2-x+5x^2,\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3cf73",
   "metadata": {
    "editable": true
   },
   "source": [
    "defined for $x\\in [-2,2]$. You can add noise if you wish.\n",
    "\n",
    "We are going to fit this function with a polynomial ansatz. The easiest thing is to set up a second-order polynomial and see if you can fit the above function.\n",
    "Feel free to play around with higher-order polynomials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a9187a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a06b51",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 1, scale your data\n",
    "\n",
    "Before fitting a regression model, it is good practice to normalize or\n",
    "standardize the features. This ensures all features are on a\n",
    "comparable scale, which is especially important when using\n",
    "regularization. Here we will perform standardization, scaling each\n",
    "feature to have mean 0 and standard deviation 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408db3d9",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 1a)\n",
    "\n",
    "Compute the mean and standard deviation of each column (feature) in your design/feature matrix $\\boldsymbol{X}$.\n",
    "Subtract the mean and divide by the standard deviation for each feature.\n",
    "\n",
    "We will also center the target $\\boldsymbol{y}$ to mean $0$. Centering $\\boldsymbol{y}$\n",
    "(and each feature) means the model does not require a separate intercept\n",
    "term, the data is shifted such that the intercept is effectively 0\n",
    ". (In practice, one could include an intercept in the model and not\n",
    "penalize it, but here we simplify by centering.)\n",
    "Choose $n=100$ data points and set up $\\boldsymbol{x}$, $\\boldsymbol{y}$ and the design matrix $\\boldsymbol{X}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41550943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the synthetic dataset\n",
    "p_features = 2\n",
    "n = 100\n",
    "x = np.linspace(-3, 3, n)\n",
    "y = 5 * x**2  - x + 2 + np.random.normal(0,0.2)\n",
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640eb2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, p, intercept=False):\n",
    "    n = len(x)\n",
    "    k = 0\n",
    "    if intercept:\n",
    "        X = np.zeros((n, p + 1))\n",
    "        X[:, 0] = 1\n",
    "        k += 1\n",
    "    else:\n",
    "        X = np.zeros((n, p))\n",
    "\n",
    "    for i in range(1, p +1):\n",
    "        X[:, i + k-1] = x**i \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37fb732c",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = polynomial_features(x, p=p_features, intercept=False)\n",
    "\n",
    "# Standardize features (zero mean, unit variance for each feature)\n",
    "X_mean = X.mean(axis=0)\n",
    "X_std = X.std(axis=0)\n",
    "\n",
    "if np.any(X_std == 0):\n",
    "    X_std[X_std == 0] = 1  # safeguard to avoid division by zero for constant features\n",
    "\n",
    "X_norm = (X - X_mean) / X_std\n",
    "X = X_norm\n",
    "# Center the target to zero mean (optional, to simplify intercept handling)\n",
    "y_mean = y.mean(axis=0)\n",
    "y_centered = y - y_mean\n",
    "y = y_centered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d861e1e3",
   "metadata": {
    "editable": true
   },
   "source": [
    "Fill in the necessary details. Do we need to center the $y$-values?\n",
    "\n",
    "After this preprocessing, each column of $\\boldsymbol{X}_{\\mathrm{norm}}$ has mean zero and standard deviation $1$\n",
    "and $\\boldsymbol{y}_{\\mathrm{centered}}$ has mean 0. This makes the optimization landscape\n",
    "nicer and ensures the regularization penalty $\\lambda \\sum_j\n",
    "\\theta_j^2$ in Ridge regression treats each coefficient fairly (since features are on the\n",
    "same scale).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9fabc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_norm: \t [5.77315973e-17 7.54951657e-17] [1. 1.]\n",
      "y_centered: \t [2.84217094e-16]\n"
     ]
    }
   ],
   "source": [
    "# Just to check that it is done correctly\n",
    "print(\"X_norm: \\t\",X_norm.mean(axis=0), X_norm.std(axis=0))\n",
    "print(\"y_centered: \\t\",y_centered.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e774d0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 2, calculate the gradients\n",
    "\n",
    "Find the gradients for OLS and Ridge regression using the mean-squared error as cost/loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6086c3e4",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{\\theta} C(\\theta) = \\frac{2}{n}\\begin{bmatrix} \\sum_{i=1}^{100} \\left(\\theta_0+\\theta_1x_i-y_i\\right) \\\\\n",
    "\\sum_{i=1}^{100}\\left( x_i (\\theta_0+\\theta_1x_i)-y_ix_i\\right) \\\\\n",
    "\\end{bmatrix} = \\frac{2}{n}X^T(X\\theta - \\mathbf{y}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_OLS(X, y,theta, eta=0.01, n=100):\n",
    "    return (2.0/n)*X.T @ (X @ theta-y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e0a42",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_\\theta C_{\\text{ridge}}(\\theta)  = \\frac{2}{n}\\begin{bmatrix} \\sum_{i=1}^{100} \\left(\\theta_0+\\theta_1x_i-y_i\\right) \\\\\n",
    "\\sum_{i=1}^{100}\\left( x_i (\\theta_0+\\theta_1x_i)-y_ix_i\\right) \\\\\n",
    "\\end{bmatrix} + 2\\lambda\\begin{bmatrix} \\theta_0 \\\\ \\theta_1\\end{bmatrix} = 2 (\\frac{1}{n}X^T(X\\theta - \\mathbf{y})+\\lambda \\theta).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d8293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_Ridge(X, y, theta, eta=0.01, lambda_param=1.0,n=100):\n",
    "\n",
    "    return (2.0/n)*X.T @ (X @ theta-y) + 2*lambda_param*theta \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dc7708",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 3, using the analytical formulae for OLS and Ridge regression to find the optimal paramters $\\boldsymbol{\\theta}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c9c86ac",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed-form Ridge coefficients: [[-1.74928386]\n",
      " [13.68402453]]\n",
      "Closed-form OLS coefficients: [[-1.74945879]\n",
      " [13.68539293]]\n"
     ]
    }
   ],
   "source": [
    "# Set regularization parameter, either a single value or a vector of values\n",
    "# Note that lambda is a python keyword. The lambda keyword is used to create small, single-expression functions without a formal name. These are often called \"anonymous functions\" or \"lambda functions.\"\n",
    "lam = 0.01\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "# Analytical form for OLS and Ridge solution: theta_Ridge = (X^T X + lambda * I)^{-1} X^T y and theta_OLS = (X^T X)^{-1} X^T y\n",
    "I = np.eye(2)\n",
    "theta_closed_formRidge = np.linalg.inv(X.T @ X + lam * np.identity(len(X.T))) @ X.T @ y\n",
    "theta_closed_formOLS = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "print(\"Closed-form Ridge coefficients:\", theta_closed_formRidge)\n",
    "print(\"Closed-form OLS coefficients:\", theta_closed_formOLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeae00fd",
   "metadata": {
    "editable": true
   },
   "source": [
    "This computes the Ridge and OLS regression coefficients directly. The identity\n",
    "matrix $I$ has the same size as $X^T X$. It adds $\\lambda$ to the diagonal of $X^T X$ for Ridge regression. We\n",
    "then invert this matrix and multiply by $X^T y$. The result\n",
    "for $\\boldsymbol{\\theta}$ is a NumPy array of shape (n$\\_$features,) containing the\n",
    "fitted parameters $\\boldsymbol{\\theta}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c215d5",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3a)\n",
    "\n",
    "Finalize, in the above code, the OLS and Ridge regression determination of the optimal parameters $\\boldsymbol{\\theta}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f909873b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Parameters (Closed-form)\n",
      "Closed-form Ridge coefficients:\n",
      " [[-1.74928386]\n",
      " [13.68402453]]\n",
      "Closed-form OLS coefficients:\n",
      " [[-1.74945879]\n",
      " [13.68539293]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal Parameters (Closed-form)\")\n",
    "print(\"Closed-form Ridge coefficients:\\n\", theta_closed_formRidge)\n",
    "print(\"Closed-form OLS coefficients:\\n\", theta_closed_formOLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587dd3dc",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3b)\n",
    "\n",
    "Explore the results as function of different values of the hyperparameter $\\lambda$. See for example exercise 4 from week 36.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2daeb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "plot_predict = []\n",
    "points = 1000\n",
    "\n",
    "lam =  np.linspace(-3, 3, points)\n",
    "\n",
    "for i in range(points):\n",
    "    theta = np.random.randn(2,1)\n",
    "    # Analytical form for OLS and Ridge solution: theta_Ridge = (X^T X + lambda * I)^{-1} X^T y and theta_OLS = (X^T X)^{-1} X^T y\n",
    "    I = np.eye(2)\n",
    "    theta_closed_formRidge = np.linalg.inv(X.T @ X + lam[i] * np.identity(len(X.T))) @ X.T @ y\n",
    "    mse_predict = mean_squared_error(y, X @ theta_closed_formRidge)\n",
    "    plot_predict.append(mse_predict)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(lam, plot_predict, label=\"MSE\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c36c5",
   "metadata": {},
   "source": [
    "We see that the model errors blows up quite fast once $\\lambda$ has i higher absolute value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa34697",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 4, Implementing the simplest form for gradient descent\n",
    "\n",
    "Alternatively, we can fit the ridge regression model using gradient\n",
    "descent. This is useful to visualize the iterative convergence and is\n",
    "necessary if $n$ and $p$ are so large that the closed-form might be\n",
    "too slow or memory-intensive. We derive the gradients from the cost\n",
    "functions defined above. Use the gradients of the Ridge and OLS cost functions with respect to\n",
    "the parameters $\\boldsymbol{\\theta}$ and set up (using the template below) your own gradient descent code for OLS and Ridge regression.\n",
    "\n",
    "Below is a template code for gradient descent implementation of ridge:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49245f55",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent OLS coefficients: [[-1.74945879]\n",
      " [13.6853929 ]]\n",
      "Gradient Descent Ridge coefficients: [[-1.74771108]\n",
      " [13.67172118]]\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent parameters, learning rate eta first\n",
    "eta = 0.0001\n",
    "lam = 0.001\n",
    "# Then number of iterations\n",
    "num_iters = 100000\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "theta_gdOLS = np.random.randn(2,1)\n",
    "theta_gdRidge = np.random.randn(2,1)\n",
    "\n",
    "# Gradient descent loop\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for OSL and Ridge\n",
    "    grad_OLS = Gradient_OLS(X_norm, y_centered, eta=eta,theta=theta_gdOLS)\n",
    "    theta_gdOLS -= eta * grad_OLS\n",
    "    grad_Ridge= Gradient_Ridge(X_norm, y_centered, eta=eta, lambda_param=lam,theta=theta_gdRidge)\n",
    "    theta_gdRidge -= eta * grad_Ridge\n",
    "\n",
    "# After the loop, theta contains the fitted coefficients\n",
    "\n",
    "print(\"Gradient Descent OLS coefficients:\", theta_gdOLS)\n",
    "print(\"Gradient Descent Ridge coefficients:\", theta_gdRidge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f43f2c",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4a)\n",
    "\n",
    "Write first a gradient descent code for OLS only using the above template.\n",
    "Discuss the results as function of the learning rate parameters and the number of iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b122bdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 0.001 , iterations: 10\n",
      "Theta OLS: [0.64795865] [0.6027791]\n",
      "\n",
      "\n",
      "Eta: 0.001 , iterations: 1000\n",
      "Theta OLS: [-1.42565275] [11.91839594]\n",
      "\n",
      "\n",
      "Eta: 0.1 , iterations: 10\n",
      "Theta OLS: [-1.4916276] [12.216093]\n",
      "\n",
      "\n",
      "Eta: 0.1 , iterations: 1000\n",
      "Theta OLS: [-1.74945879] [13.68539293]\n",
      "\n",
      "\n",
      "Eta: 10.0 , iterations: 10\n",
      "Theta OLS: [6.0253295e+12] [-9.00636199e+13]\n",
      "\n",
      "\n",
      "Eta: 10.0 , iterations: 1000\n",
      "Theta OLS: [nan] [nan]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton\\AppData\\Local\\Temp\\ipykernel_20124\\871024197.py:2: RuntimeWarning: overflow encountered in matmul\n",
      "  gradient = (2.0/n)*X.T @ (X @ theta-y)\n",
      "C:\\Users\\Anton\\AppData\\Local\\Temp\\ipykernel_20124\\871024197.py:2: RuntimeWarning: invalid value encountered in matmul\n",
      "  gradient = (2.0/n)*X.T @ (X @ theta-y)\n"
     ]
    }
   ],
   "source": [
    "n_iter = 1000\n",
    "etas =  np.logspace(-3, 1, 3)\n",
    "\n",
    "iterations = [10, 1000]\n",
    "\n",
    "for eta in etas:\n",
    "    theta_gdOLS = np.random.randn(2,1)\n",
    "    for n_iter in iterations:\n",
    "        for i in range(n_iter):\n",
    "            grad_OLS = Gradient_OLS(X_norm, y_centered, eta=eta,theta=theta_gdOLS)\n",
    "            theta_gdOLS -= eta * grad_OLS\n",
    "        print(\"Eta:\", eta, \", iterations:\", n_iter)\n",
    "        print(\"Theta OLS:\", theta_gdOLS[0], theta_gdOLS[1])\n",
    "        print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b8d8b",
   "metadata": {},
   "source": [
    "Again we see that the when eta becomes too large the model blows up, while when eta is too small it takes a long time to learn what the optimal parameters are. Meaning that the cost for finding the optimal solution is higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba303be",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4b)\n",
    "\n",
    "Write then a similar code for Ridge regression using the above template.\n",
    "Try to add a stopping parameter as function of the number iterations and the difference between the new and old $\\theta$ values. How would you define a stopping criterion?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4dda39a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 1e-05 , lambda 1e-05 , iterations: 10\n",
      "Theta Ridge: [-1.22159502] [2.0798811]\n",
      "\n",
      "\n",
      "Eta: 1e-05 , lambda 1e-05 , iterations: 10000\n",
      "Theta Ridge: [-1.31727904] [4.18361669]\n",
      "\n",
      "\n",
      "Eta: 1e-05 , lambda 0.0031622776601683794 , iterations: 10\n",
      "Theta Ridge: [-1.31736464] [4.18551423]\n",
      "\n",
      "\n",
      "Eta: 1e-05 , lambda 0.0031622776601683794 , iterations: 10000\n",
      "Theta Ridge: [-1.39491187] [5.90464085]\n",
      "\n",
      "\n",
      "Eta: 1e-05 , lambda 1.0 , iterations: 10\n",
      "Theta Ridge: [-1.39470383] [5.90501601]\n",
      "\n",
      "\n",
      "Eta: 1e-05 , lambda 1.0 , iterations: 10000\n",
      "Theta Ridge: [-1.2232759] [6.21415549]\n",
      "\n",
      "\n",
      "Eta: 0.01 , lambda 1e-05 , iterations: 10\n",
      "Theta Ridge: [0.52465994] [2.90308001]\n",
      "\n",
      "\n",
      "Converged after 846 iterations\n",
      "Eta: 0.01 , lambda 1e-05 , iterations: 10000\n",
      "Theta Ridge: [-1.74944121] [13.68525567]\n",
      "\n",
      "\n",
      "Eta: 0.01 , lambda 0.0031622776601683794 , iterations: 10\n",
      "Theta Ridge: [-1.74843272] [13.67736652]\n",
      "\n",
      "\n",
      "Converged after 557 iterations\n",
      "Eta: 0.01 , lambda 0.0031622776601683794 , iterations: 10000\n",
      "Theta Ridge: [-1.74394401] [13.64225278]\n",
      "\n",
      "\n",
      "Eta: 0.01 , lambda 1.0 , iterations: 10\n",
      "Theta Ridge: [-1.45261164] [11.36326341]\n",
      "\n",
      "\n",
      "Converged after 413 iterations\n",
      "Eta: 0.01 , lambda 1.0 , iterations: 10000\n",
      "Theta Ridge: [-0.87472942] [6.84269668]\n",
      "\n",
      "\n",
      "Eta: 10.0 , lambda 1e-05 , iterations: 10\n",
      "Theta Ridge: [1.02897634e+13] [-7.66389246e+13]\n",
      "\n",
      "\n",
      "Eta: 10.0 , lambda 1e-05 , iterations: 10000\n",
      "Theta Ridge: [nan] [nan]\n",
      "\n",
      "\n",
      "Eta: 10.0 , lambda 0.0031622776601683794 , iterations: 10\n",
      "Theta Ridge: [nan] [nan]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton\\AppData\\Local\\Temp\\ipykernel_18124\\2281413180.py:3: RuntimeWarning: overflow encountered in matmul\n",
      "  return (2.0/n)*X.T @ (X @ theta-y) + 2*lambda_param*theta\n",
      "C:\\Users\\Anton\\AppData\\Local\\Temp\\ipykernel_18124\\2281413180.py:3: RuntimeWarning: invalid value encountered in matmul\n",
      "  return (2.0/n)*X.T @ (X @ theta-y) + 2*lambda_param*theta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 10.0 , lambda 0.0031622776601683794 , iterations: 10000\n",
      "Theta Ridge: [nan] [nan]\n",
      "\n",
      "\n",
      "Eta: 10.0 , lambda 1.0 , iterations: 10\n",
      "Theta Ridge: [nan] [nan]\n",
      "\n",
      "\n",
      "Eta: 10.0 , lambda 1.0 , iterations: 10000\n",
      "Theta Ridge: [nan] [nan]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_iter = 1000\n",
    "etas =  np.logspace(-5, 1, 3)\n",
    "\n",
    "lam_params = np.logspace(-5, 0, 3)\n",
    "iterations = [10, 10000]\n",
    "\n",
    "for eta in etas:\n",
    "    theta = np.random.randn(2,1)\n",
    "    for lam in lam_params:\n",
    "        for n_iter in iterations:\n",
    "            for i in range(n_iter):\n",
    "                grad = Gradient_Ridge(X_norm, y_centered, eta=eta,theta=theta, lambda_param=lam)\n",
    "                if np.absolute(eta*grad).sum() < 1e-8:\n",
    "                    print(\"Converged after\", i, \"iterations\")\n",
    "                    break\n",
    "                theta -= eta * grad\n",
    "            \n",
    "            print(\"Eta:\", eta,\", lambda\",lam, \", iterations:\", n_iter)\n",
    "            print(\"Theta Ridge:\", theta[0], theta[1])\n",
    "            print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78362c6c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 5, Ridge regression and a new Synthetic Dataset\n",
    "\n",
    "We create a synthetic linear regression dataset with a sparse\n",
    "underlying relationship. This means we have many features but only a\n",
    "few of them actually contribute to the target. In our example, we’ll\n",
    "use 10 features with only 3 non-zero weights in the true model. This\n",
    "way, the target is generated as a linear combination of a few features\n",
    "(with known coefficients) plus some random noise. The steps we include are:\n",
    "\n",
    "Decide on the number of samples and features (e.g. 100 samples, 10 features).\n",
    "Define the **true** coefficient vector with mostly zeros (for sparsity). For example, we set $\\hat{\\boldsymbol{\\theta}} = [5.0, -3.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0]$, meaning only features 0, 1, and 6 have a real effect on y.\n",
    "\n",
    "Then we sample feature values for $\\boldsymbol{X}$ randomly (e.g. from a normal distribution). We use a normal distribution so features are roughly centered around 0.\n",
    "Then we compute the target values $y$ using the linear combination $\\boldsymbol{X}\\hat{\\boldsymbol{\\theta}}$ and add some noise (to simulate measurement error or unexplained variance).\n",
    "\n",
    "Below is the code to generate the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8be1cebe",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define dataset size\n",
    "n_samples = 100\n",
    "n_features = 10\n",
    "\n",
    "# Define true coefficients (sparse linear relationship)\n",
    "theta_true = np.array([5.0, -3.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "# Generate feature matrix X (n_samples x n_features) with random values\n",
    "X = np.random.randn(n_samples, n_features)  # standard normal distribution\n",
    "\n",
    "# Generate target values y with a linear combination of X and theta_true, plus noise\n",
    "noise = 0.5 * np.random.randn(n_samples)  # Gaussian noise\n",
    "y = X @ theta_true + noise\n",
    "y = y.reshape(-1,1)\n",
    "y_true = X @ theta_true \n",
    "y_true = y_true.reshape(-1,1)\n",
    "theta_true = theta_true.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bb695a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.],\n",
       "       [-3.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2693666",
   "metadata": {
    "editable": true
   },
   "source": [
    "This code produces a dataset where only features 0, 1, and 6\n",
    "significantly influence $\\boldsymbol{y}$. The rest of the features have zero true\n",
    "coefficient. For example, feature 0 has\n",
    "a true weight of 5.0, feature 1 has -3.0, and feature 6 has 2.0, so\n",
    "the expected relationship is:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc954d12",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "y \\approx 5 \\times x_0 \\;-\\; 3 \\times x_1 \\;+\\; 2 \\times x_6 \\;+\\; \\text{noise}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6534b610",
   "metadata": {
    "editable": true
   },
   "source": [
    "You can remove the noise if you wish to.\n",
    "\n",
    "Try to fit the above data set using OLS and Ridge regression with the analytical expressions and your own gradient descent codes.\n",
    "\n",
    "If everything worked correctly, the learned coefficients should be\n",
    "close to the true values [5.0, -3.0, 0.0, …, 2.0, …] that we used to\n",
    "generate the data. Keep in mind that due to regularization and noise,\n",
    "the learned values will not exactly equal the true ones, but they\n",
    "should be in the same ballpark. Which method (OLS or Ridge) gives the best results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c72aa561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Parameters (Closed-form)\n",
      "Closed-form OLS coefficients:\n",
      " [[ 5.00905318e+00]\n",
      " [-3.00383337e+00]\n",
      " [-1.62718294e-02]\n",
      " [ 1.44819819e-01]\n",
      " [-7.16006510e-02]\n",
      " [-4.29656382e-02]\n",
      " [ 2.05558117e+00]\n",
      " [ 1.97583716e-03]\n",
      " [ 4.11922237e-02]\n",
      " [-5.10225177e-02]]\n",
      "Closed-form Ridge coefficients:\n",
      " [[ 5.00410898e+00]\n",
      " [-2.99968373e+00]\n",
      " [-1.63065915e-02]\n",
      " [ 1.45477818e-01]\n",
      " [-7.25435409e-02]\n",
      " [-4.37776623e-02]\n",
      " [ 2.05285140e+00]\n",
      " [ 2.26563823e-03]\n",
      " [ 4.02214213e-02]\n",
      " [-5.10893259e-02]]\n",
      "\n",
      "\n",
      "Difference between true value with noise OLS, Ridge:  0.4383162375780215 0.42895864891638424\n",
      "Difference between true value OLS without noise, Ridge:  2.706168622523819e-15 0.015904259027352978\n"
     ]
    }
   ],
   "source": [
    "lam = 0.1\n",
    "#Analytical Solution\n",
    "theta_closed_formOLS = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "theta_closed_formRidge = np.linalg.inv(X.T @ X + lam * np.identity(len(X.T))) @ X.T @ y\n",
    "\n",
    "theta_closed_formOLS_true = np.linalg.inv(X.T @ X) @ X.T @ y_true\n",
    "theta_closed_formRidge_true = np.linalg.inv(X.T @ X + lam * np.identity(len(X.T))) @ X.T @ y_true\n",
    "\n",
    "print(\"Optimal Parameters (Closed-form)\")\n",
    "print(\"Closed-form OLS coefficients:\\n\", theta_closed_formOLS)\n",
    "print(\"Closed-form Ridge coefficients:\\n\", theta_closed_formRidge)\n",
    "print(\"\\n\")\n",
    "print(\"Difference between true value with noise OLS, Ridge: \",np.abs(theta_closed_formOLS - theta_true).sum(), np.abs(theta_closed_formRidge - theta_true).sum())\n",
    "print(\"Difference between true value OLS without noise, Ridge: \",np.abs(theta_closed_formOLS_true - theta_true).sum(), np.abs(theta_closed_formRidge_true - theta_true).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f52af",
   "metadata": {},
   "source": [
    "In the analytical case the Ridge seems to give the best values when we include noise but it is still very close, while if we exclude noise OLS gives the best results without a doubt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b0d528d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent OLS coefficients: [[ 5.00905252e+00]\n",
      " [-3.00382578e+00]\n",
      " [-1.62739389e-02]\n",
      " [ 1.44827775e-01]\n",
      " [-7.16050075e-02]\n",
      " [-4.29733199e-02]\n",
      " [ 2.05556653e+00]\n",
      " [ 1.97484293e-03]\n",
      " [ 4.11918375e-02]\n",
      " [-5.10178345e-02]]\n",
      "Gradient Descent Ridge coefficients: [[ 4.96007396e+00]\n",
      " [-2.96287251e+00]\n",
      " [-1.66154179e-02]\n",
      " [ 1.51238957e-01]\n",
      " [-8.08084651e-02]\n",
      " [-5.09123527e-02]\n",
      " [ 2.02862591e+00]\n",
      " [ 4.83525687e-03]\n",
      " [ 3.16594815e-02]\n",
      " [-5.16761120e-02]]\n",
      "\n",
      "\n",
      "Difference between true value with noise OLS, Ridge:  0.4383093883217313 0.4934254850344568\n",
      "Difference between true value OLS without noise, Ridge:  5.538304778910893e-05 0.15685855841264412\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent parameters, learning rate eta first\n",
    "eta = 0.01\n",
    "lam = 0.01\n",
    "# Then number of iterations\n",
    "num_iters = 1000\n",
    "\n",
    "# Initialize weights for gradient descent\n",
    "theta_gdOLS = np.random.randn(10,1)\n",
    "theta_gdOLS_true = np.random.randn(10,1)\n",
    "theta_gdRidge = np.random.randn(10,1)\n",
    "theta_gdRidge_true = np.random.randn(10,1)\n",
    "\n",
    "# Gradient descent loop\n",
    "for t in range(num_iters):\n",
    "    # Compute gradients for OSL and Ridge\n",
    "    grad_OLS = Gradient_OLS(X, y, eta=eta,theta=theta_gdOLS)\n",
    "    grad_OLS_true = Gradient_OLS(X, y_true, eta=eta,theta=theta_gdOLS_true)\n",
    "    theta_gdOLS -= eta * grad_OLS\n",
    "    theta_gdOLS_true -= eta * grad_OLS_true\n",
    "    grad_Ridge= Gradient_Ridge(X, y, eta=eta, lambda_param=lam,theta=theta_gdRidge)\n",
    "    grad_Ridge_true= Gradient_Ridge(X, y_true, eta=eta, lambda_param=lam,theta=theta_gdRidge_true)\n",
    "    theta_gdRidge -= eta * grad_Ridge\n",
    "    theta_gdRidge_true -= eta * grad_Ridge_true\n",
    "\n",
    "# After the loop, theta contains the fitted coefficients\n",
    "\n",
    "print(\"Gradient Descent OLS coefficients:\", theta_gdOLS)\n",
    "print(\"Gradient Descent Ridge coefficients:\", theta_gdRidge)\n",
    "print(\"\\n\")\n",
    "print(\"Difference between true value with noise OLS, Ridge: \",np.abs(theta_gdOLS - theta_true).sum(), np.abs(theta_gdRidge - theta_true).sum())\n",
    "print(\"Difference between true value OLS without noise, Ridge: \",np.abs(theta_gdOLS_true - theta_true).sum(), np.abs(theta_gdRidge_true - theta_true).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc0f8d9",
   "metadata": {},
   "source": [
    "Which is equivalent to what we found in the analytical case, except there has been done a lot more computation and the solutions are still not as good yet. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
