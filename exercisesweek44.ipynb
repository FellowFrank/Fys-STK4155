{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f7cd56",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html exercisesweek44.do.txt  -->\n",
    "<!-- dom:TITLE: Exercises week 44 -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c83276",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Exercises week 44\n",
    "\n",
    "**October 27-31, 2025**\n",
    "\n",
    "Date: **Deadline is Friday October 31 at midnight**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a26983",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Overarching aims of the exercises this week\n",
    "\n",
    "The exercise set this week has two parts.\n",
    "\n",
    "1. The first is a version of the exercises from week 39, where you got started with the report and github repository for project 1, only this time for project 2. This part is required, and a short feedback to this exercise will be available before the project deadline. And you can reuse these elements in your final report.\n",
    "\n",
    "2. The second is a list of questions meant as a summary of many of the central elements we have discussed in connection with projects 1 and 2, with a slight bias towards deep learning methods and their training. The hope is that these exercises can be of use in your discussions about the neural network results in project 2. **You don't need to answer all the questions, but you should be able to answer them by the end of working on project 2.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c58e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Deliverables\n",
    "\n",
    "First, join a group in canvas with your group partners. Pick an avaliable group for Project 2 in the “People” page. If you don't have a group, you should really consider joining one!\n",
    "\n",
    "Complete exercise 1 while working in an Overleaf project. Then, in canvas, include\n",
    "\n",
    "- An exported PDF of the report draft you have been working on.\n",
    "- A comment linking to the github repository used in exercise **1d)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f65f6e",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "\n",
    "Following the same directions as in the weekly exercises for week 39:\n",
    "\n",
    "**a)** Create a report document in Overleaf, and write a suitable abstract and introduction for project 2.\n",
    "\n",
    "**b)** Add a figure in your report of a heatmap showing the test accuracy of a neural network with [0, 1, 2, 3] hidden layers and [5, 10, 25, 50] nodes per hidden layer.\n",
    "\n",
    "**c)** Add a figure in your report which meets as few requirements as possible of what we consider a good figure in this course, while still including some results, a title, figure text, and axis labels. Describe in the text of the report the different ways in which the figure is lacking. (This should not be included in the final report for project 2.)\n",
    "\n",
    "**d)** Create a github repository or folder in a repository with all the elements described in exercise 4 of the weekly exercises of week 39.\n",
    "\n",
    "**e)** If applicable, add references to your report for the source of your data for regression and classification, the source of claims you make about your data, and for the sources of the gradient optimizers you use and your general claims about these.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff53b8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 2:\n",
    "\n",
    "**a)** Linear and logistic regression methods\n",
    "\n",
    "1. What is the main difference between ordinary least squares and Ridge regression?\n",
    "\n",
    "One uses only the mse to calculate and update its parameters while the other one penalize inputs that does give any valualbe information to the output. By doing this penalization it will generalize better than OLS, at least when there are parameters that dont have much value and this will also imply that it is less likley to overfit. \n",
    "\n",
    "\n",
    "2. Which kind of data set would you use logistic regression for?\n",
    "\n",
    "I would use it for discreet or categorical outcomes, datasets where we want to predict what happens and not what the specific cost is. \n",
    "\n",
    "\n",
    "3. In linear regression you assume that your output is described by a continuous non-stochastic function $f(x)$. Which is the equivalent function in logistic regression?\n",
    "\n",
    "The assumption in logistic regression would be a non-stochastic probabilistic function, a sigmoid function. That gives the probability of this being true or false.\n",
    "\n",
    "\n",
    "4. Can you find an analytic solution to a logistic regression type of problem?\n",
    "\n",
    "No, as the function we are trying to minimize becomes a collection of non-linear equations which has no closed form or analytic solutions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5. What kind of cost function would you use in logistic regression?\n",
    "\n",
    "I would use the logloss which incorporates the probabilities of it being correct or false, with an emphasis on penalizing large desrpencies in probabilities. i.e. if prediction in 0 and the actual value is 1 then a larger value if the prediction was 0.8.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a056a4",
   "metadata": {
    "editable": true
   },
   "source": [
    "**b)** Deep learning\n",
    "\n",
    "1. What is an activation function and discuss the use of an activation function? Explain three different types of activation functions?\n",
    "\n",
    "The activation function is something that introduces nonlinearity into the neural network. It is a layer after the linear regression where we get the final output of the node that is fed forward. We have gone through threee different functions which is sigmoid, gives values between 0 and 1 and plateaus for large numbers. ReLu that grows linear in the positive axes but is 0 in the negative axis. Lastly Leaky relu that seeks to fix the problem with relu where negative numbers gives zero information and makes it harder to train by multiplying the negative linear function with a small number.\n",
    "\n",
    "2. Describe the architecture of a typical feed forward Neural Network (NN).\n",
    "\n",
    "Input, Hidden layers with activation functions and by varying neurons in each layer. That finally feeds forward to the output layer by a last activation function.\n",
    "\n",
    "3. You are using a deep neural network for a prediction task. After training your model, you notice that it is strongly overfitting the training set and that the performance on the test isn’t good. What can you do to reduce overfitting?\n",
    "\n",
    "Incorporate higher penalization in the regularization for the different activation functions or the cost function. scale the input data if not already done. reduce the numbers of hidden layers or neurons.  More data.\n",
    "\n",
    "\n",
    "4. How would you know if your model is suffering from the problem of exploding gradients?\n",
    "Loss becomes Nan or infinity, unstable weights.\n",
    "\n",
    "\n",
    "\n",
    "5. Can you name and explain a few hyperparameters used for training a neural network?\n",
    "\n",
    "Training rate for how much the different parameters are changed by in each iteration, number of layers and nodes in each layer, regularization term if that is included, activation parameters like the alpha in leaky ReLu,\n",
    "\n",
    "\n",
    "\n",
    "6. Describe the architecture of a typical Convolutional Neural Network (CNN)\n",
    "Convolution layer (creating feature maps by edges and other), Pooling layer (shrinking the feature maps), 1D feed forward network, output layer\n",
    "\n",
    "\n",
    "7. What is the vanishing gradient problem in Neural Networks and how to fix it?\n",
    "\n",
    "By using relu which has a constat derivative equal to 1, this will make the multiplications become less likely to vanish because of multiple small number multiplications.\n",
    "\n",
    "\n",
    "8. When it comes to training an artificial neural network, what could the reason be for why the cost/loss doesn't decrease in a few epochs?\n",
    "Stuck in a local minimum, too low training rate, too high training rate, Poor initiazation.\n",
    "\n",
    "\n",
    "9. How does L1/L2 regularization affect a neural network?\n",
    "\n",
    "The same as for the Linear regression case. \n",
    "\n",
    "L2 making the weight become more diffused and preventing any one specific weight to be ruling.\n",
    "\n",
    "L1 Pushes many of the weights to become 0 and creating a sparse network.\n",
    "\n",
    "\n",
    "\n",
    "10. What is(are) the advantage(s) of deep learning over traditional methods like linear regression or logistic regression?\n",
    "\n",
    "The network has greater adaptability than a standard linear regression method, which one can see in the broad arrange of problems it has been put to solve. If one don't know what the specific end function should be or any other reasoning to have an other more specialized method be the solution the adaptability of a neural network will be a good choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48bc09",
   "metadata": {
    "editable": true
   },
   "source": [
    "**c)** Optimization part\n",
    "\n",
    "1. Which is the basic mathematical root-finding method behind essentially all gradient descent approaches(stochastic and non-stochastic)?\n",
    "\n",
    "Newtons descent finding the gradient and walking towards a minimum\n",
    "\n",
    "2. And why don't we use it? Or stated differently, why do we introduce the learning rate as a parameter?\n",
    "\n",
    "To have better stability and control over not over emphasising one \"direction\", and steering at a slow and steady rate.\n",
    "\n",
    "3. What might happen if you set the momentum hyperparameter too close to 1 (e.g., 0.9999) when using an optimizer for the learning rate?\n",
    "\n",
    "\n",
    "\n",
    "4. Why should we use stochastic gradient descent instead of plain gradient descent?\n",
    "\n",
    "It is easier to calculate a subset than the whole set, making it easier to scale upwards for larger datasets.\n",
    "\n",
    "\n",
    "5. Which parameters would you need to tune when use a stochastic gradient descent approach?\n",
    "\n",
    "The batch size for the stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0b5f6",
   "metadata": {
    "editable": true
   },
   "source": [
    "**d)** Analysis of results\n",
    "\n",
    "1. How do you assess overfitting and underfitting?\n",
    "\n",
    "2. Why do we divide the data in test and train and/or eventually validation sets?\n",
    "\n",
    "3. Why would you use resampling methods in the data analysis? Mention some widely popular resampling methods.\n",
    "\n",
    "4. Why might a model that does not overfit the data (maybe because there is a lot of data) perform worse when we add regularization?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
